{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "from httplib2 import Http\n",
    "from oauth2client import file, client, tools\n",
    "import io \n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/Thomas/Google Drive/Projects/Google_code_n_Oauth/Client_Oauth/Google_auth/valid-pagoda-132423-c6ac84b41833.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oauth(path, version):\n",
    "    \"\"\"\n",
    "    get authorization to create create the spreadsheet\n",
    "    Currently, version 4 of Google API\n",
    "    \"\"\"\n",
    "    SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "    path_token = path+\"token.json\"\n",
    "    path_credential = path + \"credentials.json\"\n",
    "    store = file.Storage(path_token)\n",
    "    creds = store.get()\n",
    "    if not creds or creds.invalid:\n",
    "        flow = client.flow_from_clientsecrets(path_credential, SCOPES)\n",
    "        creds = tools.run_flow(flow, store)\n",
    "    if version ==4:\n",
    "        service = build('sheets', 'v4', http=creds.authorize(Http()))\n",
    "    else:\n",
    "        service = build('drive', 'v3', http=creds.authorize(Http()))\n",
    "    return service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_extra_files(path_credential, path_to_save, dic_id):\n",
    "    drive_service = get_oauth(path = path_credential, version = 3)\n",
    "    for i in range(0,4):\n",
    "        request = drive_service.files().export_media(fileId=dic_id['id'][i],\n",
    "                                             mimeType='text/csv')\n",
    "        destination = path_to_save + '/' + dic_id['name'][i]\n",
    "        fh = io.FileIO(destination, mode='w')\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        done = False\n",
    "        while done is False:\n",
    "            status, done = downloader.next_chunk()\n",
    "            print(\"Download %d%%.\" % int(status.progress() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print('Blob {} downloaded to {}.'.format(\n",
    "        source_blob_name,\n",
    "        destination_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, destination_blob_name,  source_file_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print('File {} uploaded to {}.'.format(\n",
    "          source_file_name,\n",
    "          destination_blob_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_bq(dataset_name, bucket_uri, name_table, sql_schema):\n",
    "    client = bigquery.Client()\n",
    "    dataset_ref = client.dataset(dataset_name)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    list_bq_schema = []\n",
    "    for sql in sql_schema:\n",
    "        list_bq_schema.append(bigquery.SchemaField(sql[0], sql[1]))\n",
    "    #job_config.autodetect = True\n",
    "    job_config.schema = list_bq_schema\n",
    "    job_config.skip_leading_rows = 1\n",
    "    # The source format defaults to CSV, so the line below is optional.\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    uri = bucket_uri\n",
    "    load_job = client.load_table_from_uri(\n",
    "    uri,\n",
    "    dataset_ref.table(name_table),\n",
    "    job_config = job_config)  # API request\n",
    "    print('Starting job {}'.format(load_job.job_id))\n",
    "    load_job.result()  # Waits for table load to complete.\n",
    "    print('Job finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_files(old_path, path, new_year = 2001, previous_year = 2000):\n",
    "    #year_to_change = str(int(year) - delta)\n",
    "    #### Change path sh\n",
    "    do_file = path + '/01_all_process.sh'\n",
    "    s = open(do_file).read()\n",
    "    s = s.replace(old_path, path)\n",
    "    s = s.replace('YEAR=' + str(previous_year), 'YEAR=' + str(new_year))\n",
    "    f = open(do_file, 'w')\n",
    "    f.write(s)\n",
    "    f.close()\n",
    "    #### change do file less than 2007 \n",
    "    do_file = path + '/inf_2007/02_create_int_files.do'\n",
    "    s = open(do_file).read()\n",
    "    s = s.replace('data'+str(previous_year), 'data'+str(new_year))\n",
    "    s = s.replace('Date = \"' + str(previous_year), 'Date = \"' +str(new_year))\n",
    "    s = s.replace('local y = ' + str(previous_year), 'local y = ' +str(new_year))\n",
    "    s = s.replace(old_path, path)\n",
    "    f = open(do_file, 'w')\n",
    "    f.write(s)\n",
    "    f.close()\n",
    "    #### Change in do-file 02\n",
    "    do_file = path + '/sup_2007/02_create_int_files.do'\n",
    "    s = open(do_file).read()\n",
    "    s = s.replace(old_path, path)\n",
    "    s = s.replace('data' + str(previous_year), 'data' + str(new_year))\n",
    "    s = s.replace('tradedata-' + str(previous_year), 'tradedata-' + str(new_year))\n",
    "    f = open(do_file, 'w')\n",
    "    f.write(s)\n",
    "    f.close()\n",
    "    #### Change in do-file 02\n",
    "    do_file = path + '/03_translate.do'\n",
    "    #s = open(do_file).read()\n",
    "    with open(path + '/03_translate.do') as f:\n",
    "        lines = f.readlines()\n",
    "        if int(new_year) >= 2007:\n",
    "            print(old_path+'/sup_2007')\n",
    "            lines[0] = 'cd ' + path+'/sup_2007 \\n'\n",
    "        else:\n",
    "            lines[0] = 'cd ' + path + '/inf_2007 \\n'\n",
    "    with open(path + '/03_translate.do', \"w\") as f:\n",
    "        f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bash_stata(path):\n",
    "    translate_command = 'bash ' + path + '/01_all_process.sh'\n",
    "    print('Begin translate', path+ '/01_translate.sh')\n",
    "    os.system(translate_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df_city_country(df, cities_path,\n",
    "                            countries_path):\n",
    "    \"\"\"\n",
    "    Extract cities name from city list\n",
    "    Match only countries in the list of countries\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cities\n",
    "    list_cities = pd.read_csv(cities_path, index_col=None)\n",
    "    # Extract cities\n",
    "    raw_list = list_cities.city.tolist()\n",
    "    df['city_prod'] = df['city'].str.extract(r\"(?=(\" + '|'.join(raw_list) +\n",
    "                                             r\"))\")\n",
    "    missing_cities_before = df['city_prod'].isnull().sum()\n",
    "    print('Sum missing cities {}'.format(missing_cities_before))\n",
    "\n",
    "    example_null = df[df['city_prod'].isnull()]['city'].unique()\n",
    "    print('Exemple of missing cities {}'.format(example_null[:5]))\n",
    "\n",
    "    list_variables_to_extract = ['address', 'Company_name']\n",
    "    df['matching_city'] = 'City_var'\n",
    "    for name in list_variables_to_extract:\n",
    "        unmatch = df[df['city_prod'].isnull()]\n",
    "        unmatch['retrieve_ciy'] = unmatch[name].str.extract(\n",
    "        r\"(?=(\" + '|'.join(raw_list) + r\"))\")\n",
    "        df.loc[unmatch['retrieve_ciy'].index, 'matching_city'] = name\n",
    "        df['city_prod'] = df['city_prod'].fillna(unmatch['retrieve_ciy'])\n",
    "        missing_cities_after = df['city_prod'].isnull().sum()\n",
    "        perc_found = 1 - missing_cities_after / missing_cities_before\n",
    "        count_retrieve = missing_cities_before - missing_cities_after\n",
    "        print('{} cities have been retrieved .\\n' \\\n",
    "          'Missing cities have now {:.1%} filled values. \\n' \\\n",
    "          'We will try to retrieve {} missing cities from another \\n' \\\n",
    "          ' list'.format(count_retrieve,\n",
    "                     perc_found,\n",
    "                     missing_cities_after))\n",
    "    ### Retrieve city from list_random which is list of autonomous region or counties\n",
    "    list_random_regex = ['省.+$', '内蒙古自治区.+$', '西藏自治区.+$', '新疆维吾尔.+$', '广西壮族.+$']\n",
    "    unmatch_1 = df[df['city_prod'].isnull()]\n",
    "    unmatch_1['city_2'] = unmatch_1['city'].str.extract(r\"(\" + '|'.join(list_random_regex) +\")\")\n",
    "\n",
    "    list_random = ['省', '内蒙古自治区', '西藏自治区', '新疆维吾尔', '广西壮族']\n",
    "    unmatch_1['city_2'] = unmatch_1['city_2'].str.replace(\"(\" + '|'.join(list_random) +\")\", '', regex=True)\n",
    "    #### List unique province\n",
    "    raw_list_province = list_cities.province.unique().tolist()\n",
    "    unmatch_1['province_'] = unmatch_1['city'].str.extract(r\"(\" + '|'.join(raw_list_province) +\")\")\n",
    "    #### replace NaN with 其他\n",
    "    unmatch_1['city_2'] = unmatch_1['city_2'].fillna('其他')\n",
    "    df.loc[unmatch['retrieve_ciy'].index, 'matching_city'] = 'another_list'\n",
    "    df['city_prod'] = df['city_prod'].fillna(unmatch_1['city_2'])\n",
    "    missing_cities_after_unmatch_1 = df['city_prod'].isnull().sum()\n",
    "    count_retrieve_list_2 = missing_cities_after - missing_cities_after_unmatch_1\n",
    "    perc_found_1 = 1 - missing_cities_after_unmatch_1 / missing_cities_before\n",
    "    print('{0} cities have been retrieved from another list. Those unmatched' \\\n",
    "          'in another list are filled qith \"其他\"\\n' \\\n",
    "          'City has now {1:.0%} filled values. \\nThere are {2} unknow cities'.format(\n",
    "          count_retrieve_list_2, perc_found_1, missing_cities_after_unmatch_1))\n",
    "    ### Drop \n",
    "    df = df.drop(columns=['Company_name', 'address', 'city'])\n",
    "    print(df.isna().sum())\n",
    "    df = df.dropna()\n",
    "    #### Merge provinces\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        list_cities,\n",
    "        how='left',\n",
    "        left_on=['city_prod'],\n",
    "        right_on=['city'],\n",
    "        suffixes=('', '_y'))\n",
    "\n",
    "    df = df.drop(columns=['city'])\n",
    "    df['province'] = df['province'].fillna(unmatch_1['province_'])\n",
    "    df['province'] = df['province'].fillna('其他')\n",
    "\n",
    "    print(df.groupby('imp_exp')['value'].sum())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_business_type_and_intermediate(df, business_path,\n",
    "                                           intermediate_path):\n",
    "    # retrieve business types\n",
    "    business = pd.read_csv(business_path, index_col=None)\n",
    "    df_nan_biz = df[df['Business_type'].isnull()]\n",
    "\n",
    "    nan_source = df_nan_biz['Business_type'].isna().sum()\n",
    "    print(\n",
    "        'There are {} missing Business type in the Dataset'.format(nan_source))\n",
    "    if nan_source !=0:\n",
    "        raw_list = business.x.tolist()\n",
    "        df_nan_biz['Business_type'] = df_nan_biz['Company_name'].str.extract(\n",
    "            r\"(?=(\" + '|'.join(raw_list) + r\"))\")\n",
    "\n",
    "        nan_after = df_nan_biz['Business_type'].isna().sum()\n",
    "\n",
    "        perc_total = (1 - nan_after / nan_source)\n",
    "        count_retrieve = nan_source - nan_after\n",
    "        shape_df = len(df)\n",
    "        print(\n",
    "            'Business type is filled at {0:.0%} with {1}' \\\n",
    "            ' values retrieve from company name.\\n The missing {2}' \\\n",
    "            ' are filling with \"其他\" \\n The dataset has {3}' \\\n",
    "            ' rows'.format(perc_total,\n",
    "                       count_retrieve,\n",
    "                       nan_after, \n",
    "                       shape_df))\n",
    "    # df_nan_biz = df_nan_biz.dropna()\n",
    "\n",
    "        df = df.fillna(df_nan_biz)\n",
    "        df['Business_type'] = df['Business_type'].fillna('其他')\n",
    "        df['Business_type'] = np.where(df['Business_type'] == ' ', '其他',\n",
    "                                   df['Business_type'])\n",
    "    \n",
    "    \n",
    "\n",
    "    # Create intermediate firms\n",
    "    intermediate = pd.read_csv(intermediate_path, index_col=None)\n",
    "    raw_list = intermediate.intermediate.tolist()\n",
    "    df['intermediate'] = df['Company_name'].str.extract(\n",
    "        r\"(?=(\" + '|'.join(raw_list) + r\"))\")\n",
    "    df['intermediate'] = df[[\n",
    "        'intermediate'\n",
    "    ]].applymap(lambda x: 'No' if pd.isnull(x) else 'Yes')\n",
    "\n",
    "    print(df.groupby('intermediate')['intermediate'].count())\n",
    "    #df = df.drop(columns= ['Company_name'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_gz(path, name, business_path, intermediate_path, cities_path,\n",
    "               countries_path):\n",
    "    allFiles = glob.glob(path + \"/*.csv\")\n",
    "    list_ = []\n",
    "\n",
    "    list_to_keep = [\n",
    "        'Date',\n",
    "        'ID',\n",
    "        'city',\n",
    "        'address',\n",
    "        'Company_name',\n",
    "        'Business_type',\n",
    "        'Trade_type',\n",
    "        'imp_exp',\n",
    "        'HS',\n",
    "        'Origin_and_destination',\n",
    "        'value',\n",
    "        'Quantity',\n",
    "    ]\n",
    "\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_, index_col=None, header=0, usecols=list_to_keep)\n",
    "        list_.append(df)\n",
    "        \n",
    "    df = pd.concat(list_, axis=0, ignore_index=True)\n",
    "\n",
    "    df = df[list_to_keep]\n",
    "    df = df[(df['imp_exp'] == '进口') | (df['imp_exp'] == '出口') ]\n",
    "    #### Print total import/export raw data\n",
    "    print(df.groupby('imp_exp')['value'].sum())\n",
    "    print(df.shape)\n",
    "\n",
    "    df_1 = prepare_business_type_and_intermediate(df, business_path,\n",
    "                                                  intermediate_path)\n",
    "\n",
    "    df_1 = prepare_df_city_country(df_1, cities_path, countries_path)\n",
    "\n",
    "    #### Keep var\n",
    "    #### Rename\n",
    "    #df_1 = df_1.rename(index=str, columns={\"origin_id\": \"Country_id\"})\n",
    "\n",
    "    #### To avoid any types problem, change type manually\n",
    "    df_1['Date'] = df_1['Date'].astype('int64')\n",
    "    df_1['ID'] = pd.to_numeric(df_1['ID'], errors='coerce')\n",
    "    rem = df_1['ID'].isna().sum()\n",
    "    print('{} rows have been removed from ID due to wrong formating'.format(rem))\n",
    "    df_1 = df_1[pd.notnull(df_1['ID'])]\n",
    "    df_1['ID'] = df_1['ID'].astype('int64')\n",
    "    #df_1['Company_name'] = df_1['Company_name'].astype('object')\n",
    "    df_1['Business_type'] = df_1['Business_type'].astype('object')\n",
    "    df_1['intermediate'] = df_1['intermediate'].astype('object')\n",
    "    df_1['Trade_type'] = df_1['Trade_type'].astype('object')\n",
    "    df_1['province'] = df_1['province'].astype('object')\n",
    "    df_1['city_prod'] = df_1['city_prod'].astype('object')\n",
    "    df_1['matching_city'] = df_1['matching_city'].astype('object')\n",
    "    df_1['imp_exp'] = df_1['imp_exp'].astype('object')\n",
    "    df_1['HS'] = df_1['HS'].astype('int64')\n",
    "    df_1['Origin_and_destination'] = df_1['Origin_and_destination'].astype(\n",
    "        'object')\n",
    "    #df_1['Country_id'] = df_1['Country_id'].astype('int64')\n",
    "    df_1['value'] = df_1['value'].astype('int64')\n",
    "    df_1['Quantity'] = df_1['Quantity'].astype('int64')\n",
    "    \n",
    "    list_aggregate = [\n",
    "        'Date', 'ID','Business_type', 'intermediate', 'Trade_type', 'province', 'city_prod','matching_city',\n",
    "        'imp_exp', 'HS', 'Origin_and_destination'\n",
    "    ]\n",
    "    list_to_keep = ['Date', 'ID','Business_type', 'intermediate', 'Trade_type', 'province', 'city_prod','matching_city',\n",
    "        'imp_exp', 'HS', 'Origin_and_destination', 'value', 'Quantity']\n",
    "    df_1 = df_1.groupby(list_aggregate).sum().reset_index()\n",
    "    df_1= df_1[list_to_keep]\n",
    "    #### Unique HS\n",
    "    unique_HS = df_1.groupby(['imp_exp'])['HS'].nunique()\n",
    "\n",
    "    #### Unique Country\n",
    "    unique_country = df_1.groupby(\n",
    "        ['imp_exp'])['Origin_and_destination'].nunique()\n",
    "\n",
    "    print('Unique HS \\n {} \\n unique countries \\n {}'.format(\n",
    "        unique_HS, unique_country))\n",
    "    \n",
    "    print(df_1.groupby('imp_exp')['value'].sum())\n",
    "\n",
    "    df_1.to_csv(\n",
    "        path + \"/\" + name,\n",
    "        sep=',',\n",
    "        header=True,\n",
    "        index=False,\n",
    "        chunksize=100000,\n",
    "        compression='gzip',\n",
    "        encoding='utf-8')\n",
    "\n",
    "    return df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressed_gz(path_file_name, file_name, bucket_name, blob_name,\n",
    "                  dataset_name, SQL_schema,business_path, intermediate_path, cities_path, countries_path):\n",
    "    \"\"\"\n",
    "    function to Compress into gz and move to the GCP + BQ\n",
    "    path_file_name: path where the csv are and where the future gz will be stored\n",
    "    file_name: Name of the gz to save\n",
    "    bucket_name: Backet name to store the data in GCS. Need to be root\n",
    "    blob_name: Name of the subfolder in the bucket\n",
    "    dataset_name: Name given to the dataset stored in BG\n",
    "    SQL_schema: Manually define SQL schema\n",
    "    cities_path: Path to open the cities list\n",
    "    countries_path: Path to open the countries list\n",
    "    \"\"\"\n",
    "    \n",
    "    #### Open all cv\n",
    "    df = save_to_gz(path_file_name,\n",
    "                    file_name,\n",
    "                    business_path,\n",
    "                    intermediate_path,\n",
    "                    cities_path,\n",
    "                    countries_path)\n",
    "    ##### Remove csv\n",
    "    #os.remove(file_name)\n",
    "    \n",
    "    #### Move to GCS\n",
    "    print('Begin to upload to GCS')\n",
    "    destination_blob_name = blob_name + '/' + file_name\n",
    "    source_file_name = path_file_name + '/' + file_name\n",
    "    upload_blob(bucket_name = bucket_name,\n",
    "            destination_blob_name = destination_blob_name,\n",
    "            source_file_name = source_file_name)\n",
    "    \n",
    "    #### Move to BG\n",
    "    print('Begin to move to BG')\n",
    "    bucket_uri = 'gs://' + bucket_name + '/' + blob_name + '/'+ file_name\n",
    "    name_table = re.findall(r\"^\\w+\", file_name)[0]\n",
    "    upload_bq(dataset_name = dataset_name, \n",
    "          bucket_uri = bucket_uri,\n",
    "          name_table = name_table,\n",
    "          sql_schema = SQL_schema)\n",
    "    \n",
    "    ##### Remove gz\n",
    "    os.remove(source_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload extra file\n",
    "\n",
    "We upload the list_intermediate, list_country, list_cities, business from Google Drive. \n",
    "\n",
    "- Filename is the rar name\n",
    "\n",
    "## Attention\n",
    "\n",
    "To run the program, the root folder needs to have:\n",
    "\n",
    "- a subfolder `translate` that contain the do files to translate the dta\n",
    "- a subfolder `extra` to host the extra file downloaded from the Drive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_path = '/Users/Thomas/Downloads/Stata_translate'\n",
    "path = '/Users/Thomas/Downloads/Stata_translate'\n",
    "path_source = path + '/' +'data_' + str(2007)\n",
    "path_save_extra = path + '/extra'\n",
    "##### Need to be set dynamically\n",
    "file_name = 'data_2007.rar'\n",
    "cities_path = '/Users/Thomas/Downloads/Stata_translate/extra/list_cities.csv'\n",
    "countries_path = '/Users/Thomas/Downloads/Stata_translate/extra/list_country.csv'\n",
    "business_path = '/Users/Thomas/Downloads/Stata_translate/extra/business.csv'\n",
    "intermediate_path = '/Users/Thomas/Downloads/Stata_translate/extra/list_intermediate_firms.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_credential = '/Users/Thomas/Google Drive/Projects/Google_code_n_Oauth/Client_Oauth/Google_auth/'\n",
    "dic_id = {\n",
    "    'id' : ['1jSjy6eCqT1ogHsRQH268thmy24HpydftpydRBL715lY',\n",
    "           '1UsjJZ-Fveujx4nvRXGE1Q1l7nbHzqxK0tBfPSdqq3iU',\n",
    "           '1i8u2h8Oh5KyDQwo6aT1km_S03topF4lRnRV9fasl51c', \n",
    "           '1DY5tCtMMw2kcbcbHI30Zo1-QYb6NmR_ZDEqQURKCc4s'],\n",
    "    'name': ['list_intermediate_firms.csv', 'list_country.csv',\n",
    "    'list_cities.csv', 'business.csv']\n",
    "}\n",
    "upload_extra_files(path_credential  = path_credential,\n",
    "                   path_to_save = path_save_extra, \n",
    "                   dic_id = dic_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload files from GCS\n",
    "\n",
    "It is better to update all the files so that we can proceed in batch\n",
    "\n",
    "We need to set filename and source blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_blob(bucket_name = 'store_data_trade',\n",
    "            source_blob_name = 'raw_data/data2008.rar',\n",
    "            destination_file_name = path + '/' + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate the data\n",
    "\n",
    "We can translate after having downloaded the dta\n",
    "\n",
    "1. First we need to change the year in the do files and sh file\n",
    "2. Translate dta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_files(old_path, path, new_year = 2000, previous_year = 2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_stata(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset and Move to the cloud\n",
    "\n",
    "The next step prepares the dataset:\n",
    "\n",
    "- retrieve business types\n",
    "- Create intermediate firms\n",
    "- Merge Cities\n",
    "- Merge provinces\n",
    "- Merge countries\n",
    "- Aggreate the data \n",
    "\n",
    "The final dataset has the following variables\n",
    "\n",
    "- 'Date'\n",
    "- 'ID'\n",
    "- 'Company_name'\n",
    "- 'Business_type'\n",
    "- 'Trade_type'\n",
    "- 'city_prod'\n",
    "- 'province'\n",
    "- 'imp_exp'\n",
    "- 'HS'\n",
    "- 'Origin_and_destination'\n",
    "- 'Country_id'\n",
    "- 'value'\n",
    "- 'Quantity'\n",
    "\n",
    "After that, the data is moved to Google Cloud Storage and Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_schema = [\n",
    "['Date','INTEGER'],\n",
    "['ID','INTEGER'],\n",
    "['Business_type','STRING'],\n",
    "['intermediate', 'STRING'],    \n",
    "['Trade_type','STRING'],\n",
    "['province','STRING'],   \n",
    "['city_prod','STRING'],\n",
    "['matching_city','STRING'],\n",
    "['imp_exp','STRING'],\n",
    "['HS','INTEGER'],\n",
    "['Origin_or_destination','STRING'],\n",
    "['value','INTEGER'],\n",
    "['Quantity','INTEGER']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_cloud = \"data_2005.gz\"\n",
    "compressed_gz(path_file_name= path,\n",
    "              file_name = file_name_cloud,\n",
    "              bucket_name = 'store_data_trade',\n",
    "              blob_name = 'gz_files',\n",
    "              dataset_name = 'trade_data',\n",
    "              SQL_schema = SQL_schema, \n",
    "              business_path = business_path,\n",
    "              intermediate_path = intermediate_path,\n",
    "              cities_path = cities_path,\n",
    "              countries_path = countries_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch mode\n",
    "\n",
    "For a batch mode, we can run the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def batch_model(list_year, download = True):\n",
    "    for i in list_year: \n",
    "        \n",
    "        start = time.time()\n",
    "        file_name = 'data_' + str(i) + '.rar'\n",
    "        #### This is for year after 2007\n",
    "        path_source = path + '/' +'data_' + str(i)\n",
    "        source_blob_name = 'raw_data/data'+ str(i) + '.rar'\n",
    "        print(file_name, '\\n',\n",
    "        source_blob_name)\n",
    "    \n",
    "        if download:\n",
    "            download_blob(bucket_name = 'store_data_trade',\n",
    "                source_blob_name = source_blob_name,\n",
    "                destination_file_name = path + '/' + file_name)\n",
    "        previous_year  = int(i) - 1\n",
    "        if int(i) > 2000:\n",
    "            translate_files(old_path, path, new_year = i, previous_year = previous_year)\n",
    "    \n",
    "        bash_stata(path)\n",
    "    \n",
    "        file_name_cloud = 'data_' + str(i) + '.gz'\n",
    "        try:\n",
    "            compressed_gz(path_file_name= path,\n",
    "              file_name = file_name_cloud,\n",
    "              bucket_name = 'store_data_trade',\n",
    "              blob_name = 'gz_files',\n",
    "              dataset_name = 'trade_data',\n",
    "              SQL_schema = SQL_schema, \n",
    "              business_path = business_path,\n",
    "              intermediate_path = intermediate_path,\n",
    "              cities_path = cities_path,\n",
    "              countries_path = countries_path)\n",
    "            end = time.time()\n",
    "            print('Year {} finished in {} minutes'.format(i, (end - start)/60))\n",
    "            \n",
    "            #### Remove csv\n",
    "            allFiles = glob.glob(path + \"/*.csv\")\n",
    "            for i in allFiles: \n",
    "                os.remove(i) \n",
    "            \n",
    "        except:\n",
    "            allFiles = glob.glob(path + \"/*.csv\")\n",
    "            for i in allFiles: \n",
    "                os.remove(i)\n",
    "                \n",
    "    f_end = time.time()\n",
    "    print((f_end - start)/360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_2003.rar \n",
      " raw_data/data2003.rar\n",
      "Blob raw_data/data2003.rar downloaded to /Users/Thomas/Downloads/Stata_translate/data_2003.rar.\n",
      "Begin translate /Users/Thomas/Downloads/Stata_translate/01_translate.sh\n",
      "imp_exp\n",
      "出口    4.371436e+11\n",
      "进口    4.096980e+11\n",
      "Name: value, dtype: float64\n",
      "(14771186, 12)\n",
      "There are 0 missing Business type in the Dataset\n",
      "intermediate\n",
      "No     9671127\n",
      "Yes    5100059\n",
      "Name: intermediate, dtype: int64\n",
      "Sum missing cities 706459\n",
      "Exemple of missing cities ['内蒙古自治区呼伦贝尔盟' '山东省其它' '河北省其它' '内蒙古自治区哲里木盟' '甘肃省其它']\n",
      "428504 cities have been retrieved .\n",
      "Missing cities have now 60.7% filled values. \n",
      "We will try to retrieve 277955 missing cities from another \n",
      " list\n",
      "630639 cities have been retrieved .\n",
      "Missing cities have now 89.3% filled values. \n",
      "We will try to retrieve 75820 missing cities from another \n",
      " list\n",
      "75820 cities have been retrieved from another list. Those unmatchedin another list are filled qith \"其他\"\n",
      "City has now 100% filled values. \n",
      "There are 0 unknow cities\n",
      "Date                      0\n",
      "ID                        0\n",
      "Business_type             0\n",
      "Trade_type                0\n",
      "imp_exp                   0\n",
      "HS                        0\n",
      "Origin_and_destination    0\n",
      "value                     0\n",
      "Quantity                  0\n",
      "intermediate              0\n",
      "city_prod                 0\n",
      "matching_city             0\n",
      "dtype: int64\n",
      "imp_exp\n",
      "出口    4.371436e+11\n",
      "进口    4.096980e+11\n",
      "Name: value, dtype: float64\n",
      "0 rows have been removed from ID due to wrong formating\n",
      "Unique HS \n",
      " imp_exp\n",
      "出口    5030\n",
      "进口    5009\n",
      "Name: HS, dtype: int64 \n",
      " unique countries \n",
      " imp_exp\n",
      "出口    229\n",
      "进口    205\n",
      "Name: Origin_and_destination, dtype: int64\n",
      "imp_exp\n",
      "出口    437143561736\n",
      "进口    409698013930\n",
      "Name: value, dtype: int64\n",
      "Begin to upload to GCS\n",
      "File /Users/Thomas/Downloads/Stata_translate/data_2003.gz uploaded to gz_files/data_2003.gz.\n",
      "Begin to move to BG\n",
      "Starting job ff526e48-0c0d-4269-8fea-26da2a4e17f2\n",
      "Job finished.\n",
      "Year 2003 finished in 41.93045495351156 minutes\n",
      "data_2004.rar \n",
      " raw_data/data2004.rar\n",
      "Blob raw_data/data2004.rar downloaded to /Users/Thomas/Downloads/Stata_translate/data_2004.rar.\n",
      "Begin translate /Users/Thomas/Downloads/Stata_translate/01_translate.sh\n",
      "imp_exp\n",
      "出口    592261956828\n",
      "进口    557808472557\n",
      "Name: value, dtype: int64\n",
      "(17381860, 12)\n",
      "There are 707 missing Business type in the Dataset\n",
      "Business type is filled at 0% with 0 values retrieve from company name.\n",
      " The missing 707 are filling with \"其他\" \n",
      " The dataset has 17381860 rows\n",
      "intermediate\n",
      "No     11815221\n",
      "Yes     5566639\n",
      "Name: intermediate, dtype: int64\n",
      "Sum missing cities 766772\n",
      "Exemple of missing cities ['河北省其它' '山西省其它' '内蒙古自治区其它' '山东省其它' '河南省其它']\n",
      "521586 cities have been retrieved .\n",
      "Missing cities have now 68.0% filled values. \n",
      "We will try to retrieve 245186 missing cities from another \n",
      " list\n",
      "646376 cities have been retrieved .\n",
      "Missing cities have now 84.3% filled values. \n",
      "We will try to retrieve 120396 missing cities from another \n",
      " list\n",
      "120396 cities have been retrieved from another list. Those unmatchedin another list are filled qith \"其他\"\n",
      "City has now 100% filled values. \n",
      "There are 0 unknow cities\n",
      "Date                      0\n",
      "ID                        0\n",
      "Business_type             0\n",
      "Trade_type                0\n",
      "imp_exp                   0\n",
      "HS                        0\n",
      "Origin_and_destination    0\n",
      "value                     0\n",
      "Quantity                  0\n",
      "intermediate              0\n",
      "city_prod                 0\n",
      "matching_city             0\n",
      "dtype: int64\n",
      "imp_exp\n",
      "出口    592261956828\n",
      "进口    557808472557\n",
      "Name: value, dtype: int64\n",
      "0 rows have been removed from ID due to wrong formating\n",
      "Unique HS \n",
      " imp_exp\n",
      "出口    5014\n",
      "进口    5019\n",
      "Name: HS, dtype: int64 \n",
      " unique countries \n",
      " imp_exp\n",
      "出口    229\n",
      "进口    210\n",
      "Name: Origin_and_destination, dtype: int64\n",
      "imp_exp\n",
      "出口    592261956828\n",
      "进口    557808472557\n",
      "Name: value, dtype: int64\n",
      "Begin to upload to GCS\n",
      "File /Users/Thomas/Downloads/Stata_translate/data_2004.gz uploaded to gz_files/data_2004.gz.\n",
      "Begin to move to BG\n",
      "Starting job 6f3a5164-a9b1-4d09-b54c-855f7bcf76ab\n",
      "Job finished.\n",
      "Year 2004 finished in 54.46799038251241 minutes\n",
      "data_2005.rar \n",
      " raw_data/data2005.rar\n",
      "Blob raw_data/data2005.rar downloaded to /Users/Thomas/Downloads/Stata_translate/data_2005.rar.\n",
      "Begin translate /Users/Thomas/Downloads/Stata_translate/01_translate.sh\n",
      "imp_exp\n",
      "出口    7.596584e+11\n",
      "进口    6.566029e+11\n",
      "Name: value, dtype: float64\n",
      "(20046608, 12)\n",
      "There are 13986 missing Business type in the Dataset\n",
      "Business type is filled at 0% with 0 values retrieve from company name.\n",
      " The missing 13986 are filling with \"其他\" \n",
      " The dataset has 20046608 rows\n",
      "intermediate\n",
      "No     14090063\n",
      "Yes     5956545\n",
      "Name: intermediate, dtype: int64\n",
      "Sum missing cities 782392\n",
      "Exemple of missing cities ['山东省其它' '山西省其它' '内蒙古自治区哲里木盟' '河北省其它' '内蒙古自治区其它']\n",
      "487329 cities have been retrieved .\n",
      "Missing cities have now 62.3% filled values. \n",
      "We will try to retrieve 295063 missing cities from another \n",
      " list\n",
      "633962 cities have been retrieved .\n",
      "Missing cities have now 81.0% filled values. \n",
      "We will try to retrieve 148430 missing cities from another \n",
      " list\n",
      "148430 cities have been retrieved from another list. Those unmatchedin another list are filled qith \"其他\"\n",
      "City has now 100% filled values. \n",
      "There are 0 unknow cities\n",
      "Date                      0\n",
      "ID                        0\n",
      "Business_type             0\n",
      "Trade_type                0\n",
      "imp_exp                   0\n",
      "HS                        0\n",
      "Origin_and_destination    0\n",
      "value                     0\n",
      "Quantity                  0\n",
      "intermediate              0\n",
      "city_prod                 0\n",
      "matching_city             0\n",
      "dtype: int64\n",
      "imp_exp\n",
      "出口    7.596584e+11\n",
      "进口    6.566029e+11\n",
      "Name: value, dtype: float64\n",
      "4 rows have been removed from ID due to wrong formating\n",
      "Unique HS \n",
      " imp_exp\n",
      "出口    5032\n",
      "进口    5036\n",
      "Name: HS, dtype: int64 \n",
      " unique countries \n",
      " imp_exp\n",
      "出口    234\n",
      "进口    207\n",
      "Name: Origin_and_destination, dtype: int64\n",
      "imp_exp\n",
      "出口    759657828467\n",
      "进口    656601943802\n",
      "Name: value, dtype: int64\n",
      "Begin to upload to GCS\n",
      "File /Users/Thomas/Downloads/Stata_translate/data_2005.gz uploaded to gz_files/data_2005.gz.\n",
      "Begin to move to BG\n",
      "Starting job 1a8dc356-1fbc-43eb-9979-4ac2259d1ddc\n",
      "Job finished.\n",
      "Year 2005 finished in 64.47802198330561 minutes\n",
      "data_2006.rar \n",
      " raw_data/data2006.rar\n",
      "Blob raw_data/data2006.rar downloaded to /Users/Thomas/Downloads/Stata_translate/data_2006.rar.\n",
      "Begin translate /Users/Thomas/Downloads/Stata_translate/01_translate.sh\n",
      "imp_exp\n",
      "出口    9.664408e+11\n",
      "进口    7.880962e+11\n",
      "Name: value, dtype: float64\n",
      "(9933642, 12)\n",
      "There are 226181 missing Business type in the Dataset\n",
      "Business type is filled at 0% with 464 values retrieve from company name.\n",
      " The missing 225717 are filling with \"其他\" \n",
      " The dataset has 9933642 rows\n",
      "intermediate\n",
      "No     5681914\n",
      "Yes    4251728\n",
      "Name: intermediate, dtype: int64\n",
      "Sum missing cities 599097\n",
      "Exemple of missing cities ['山西省其它' '陕西省其它' '广东省其它' '湖南省其它' '内蒙古自治区其它']\n",
      "247652 cities have been retrieved .\n",
      "Missing cities have now 41.3% filled values. \n",
      "We will try to retrieve 351445 missing cities from another \n",
      " list\n",
      "509362 cities have been retrieved .\n",
      "Missing cities have now 85.0% filled values. \n",
      "We will try to retrieve 89735 missing cities from another \n",
      " list\n",
      "89735 cities have been retrieved from another list. Those unmatchedin another list are filled qith \"其他\"\n",
      "City has now 100% filled values. \n",
      "There are 0 unknow cities\n",
      "Date                      0\n",
      "ID                        0\n",
      "Business_type             0\n",
      "Trade_type                0\n",
      "imp_exp                   0\n",
      "HS                        0\n",
      "Origin_and_destination    0\n",
      "value                     0\n",
      "Quantity                  0\n",
      "intermediate              0\n",
      "city_prod                 0\n",
      "matching_city             0\n",
      "dtype: int64\n",
      "imp_exp\n",
      "出口    9.664408e+11\n",
      "进口    7.880962e+11\n",
      "Name: value, dtype: float64\n",
      "0 rows have been removed from ID due to wrong formating\n",
      "Unique HS \n",
      " imp_exp\n",
      "出口    5029\n",
      "进口    5047\n",
      "Name: HS, dtype: int64 \n",
      " unique countries \n",
      " imp_exp\n",
      "出口    233\n",
      "进口    216\n",
      "Name: Origin_and_destination, dtype: int64\n",
      "imp_exp\n",
      "出口    966440808688\n",
      "进口    788096236952\n",
      "Name: value, dtype: int64\n",
      "Begin to upload to GCS\n",
      "File /Users/Thomas/Downloads/Stata_translate/data_2006.gz uploaded to gz_files/data_2006.gz.\n",
      "Begin to move to BG\n",
      "Starting job 3b47a12b-d8ab-4e10-a82d-e3007a8734c5\n",
      "Job finished.\n",
      "Year 2006 finished in 61.124303865432736 minutes\n",
      "data_2007.rar \n",
      " raw_data/data2007.rar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob raw_data/data2007.rar downloaded to /Users/Thomas/Downloads/Stata_translate/data_2007.rar.\n",
      "/Users/Thomas/Downloads/Stata_translate/sup_2007\n",
      "Begin translate /Users/Thomas/Downloads/Stata_translate/01_translate.sh\n",
      "imp_exp\n",
      "出口    1.216709e+12\n",
      "进口    9.524580e+11\n",
      "Name: value, dtype: float64\n",
      "(10491595, 12)\n",
      "There are 1136841 missing Business type in the Dataset\n",
      "Business type is filled at 83% with 942152 values retrieve from company name.\n",
      " The missing 194689 are filling with \"其他\" \n",
      " The dataset has 10491595 rows\n",
      "intermediate\n",
      "No     6230369\n",
      "Yes    4261226\n",
      "Name: intermediate, dtype: int64\n",
      "Sum missing cities 496912\n",
      "Exemple of missing cities ['广东省番禹' '辽宁省沉阳' '安徽省宣城' '浙江省其它' '广西壮族其它']\n",
      "221919 cities have been retrieved .\n",
      "Missing cities have now 44.7% filled values. \n",
      "We will try to retrieve 274993 missing cities from another \n",
      " list\n",
      "446145 cities have been retrieved .\n",
      "Missing cities have now 89.8% filled values. \n",
      "We will try to retrieve 50767 missing cities from another \n",
      " list\n",
      "50767 cities have been retrieved from another list. Those unmatchedin another list are filled qith \"其他\"\n",
      "City has now 100% filled values. \n",
      "There are 0 unknow cities\n",
      "Date                      0\n",
      "ID                        0\n",
      "Business_type             0\n",
      "Trade_type                0\n",
      "imp_exp                   0\n",
      "HS                        0\n",
      "Origin_and_destination    0\n",
      "value                     0\n",
      "Quantity                  0\n",
      "intermediate              0\n",
      "city_prod                 0\n",
      "matching_city             0\n",
      "dtype: int64\n",
      "imp_exp\n",
      "出口    1.216709e+12\n",
      "进口    9.524580e+11\n",
      "Name: value, dtype: float64\n",
      "0 rows have been removed from ID due to wrong formating\n",
      "Unique HS \n",
      " imp_exp\n",
      "出口    4960\n",
      "进口    4946\n",
      "Name: HS, dtype: int64 \n",
      " unique countries \n",
      " imp_exp\n",
      "出口    234\n",
      "进口    218\n",
      "Name: Origin_and_destination, dtype: int64\n",
      "imp_exp\n",
      "出口    1216708964537\n",
      "进口     952458017870\n",
      "Name: value, dtype: int64\n",
      "Begin to upload to GCS\n",
      "File /Users/Thomas/Downloads/Stata_translate/data_2007.gz uploaded to gz_files/data_2007.gz.\n",
      "Begin to move to BG\n",
      "Starting job 88ab48ea-d7bc-42b1-9549-e2efbe05f693\n",
      "Job finished.\n",
      "Year 2007 finished in 48.331952218214674 minutes\n",
      "data_2008.rar \n",
      " raw_data/data2008.rar\n",
      "Blob raw_data/data2008.rar downloaded to /Users/Thomas/Downloads/Stata_translate/data_2008.rar.\n",
      "/Users/Thomas/Downloads/Stata_translate/sup_2007\n",
      "Begin translate /Users/Thomas/Downloads/Stata_translate/01_translate.sh\n",
      "imp_exp\n",
      "出口    1.427238e+12\n",
      "进口    1.126655e+12\n",
      "Name: value, dtype: float64\n",
      "(11056702, 12)\n",
      "There are 2238256 missing Business type in the Dataset\n",
      "Business type is filled at 94% with 2097263 values retrieve from company name.\n",
      " The missing 140993 are filling with \"其他\" \n",
      " The dataset has 11056702 rows\n",
      "intermediate\n",
      "No     6336802\n",
      "Yes    4719900\n",
      "Name: intermediate, dtype: int64\n",
      "Sum missing cities 2489573\n",
      "Exemple of missing cities ['广东省番禹' nan '辽宁省沉阳' '浙江省其它' '广西壮族其它']\n",
      "919898 cities have been retrieved .\n",
      "Missing cities have now 37.0% filled values. \n",
      "We will try to retrieve 1569675 missing cities from another \n",
      " list\n",
      "2346524 cities have been retrieved .\n",
      "Missing cities have now 94.3% filled values. \n",
      "We will try to retrieve 143049 missing cities from another \n",
      " list\n",
      "143049 cities have been retrieved from another list. Those unmatchedin another list are filled qith \"其他\"\n",
      "City has now 100% filled values. \n",
      "There are 0 unknow cities\n",
      "Date                      0\n",
      "ID                        0\n",
      "Business_type             0\n",
      "Trade_type                0\n",
      "imp_exp                   0\n",
      "HS                        0\n",
      "Origin_and_destination    0\n",
      "value                     0\n",
      "Quantity                  0\n",
      "intermediate              0\n",
      "city_prod                 0\n",
      "matching_city             0\n",
      "dtype: int64\n",
      "imp_exp\n",
      "出口    1.427238e+12\n",
      "进口    1.126655e+12\n",
      "Name: value, dtype: float64\n",
      "0 rows have been removed from ID due to wrong formating\n",
      "Unique HS \n",
      " imp_exp\n",
      "出口    4906\n",
      "进口    4925\n",
      "Name: HS, dtype: int64 \n",
      " unique countries \n",
      " imp_exp\n",
      "出口    232\n",
      "进口    219\n",
      "Name: Origin_and_destination, dtype: int64\n",
      "imp_exp\n",
      "出口    1427237591564\n",
      "进口    1126655358311\n",
      "Name: value, dtype: int64\n",
      "Begin to upload to GCS\n",
      "File /Users/Thomas/Downloads/Stata_translate/data_2008.gz uploaded to gz_files/data_2008.gz.\n",
      "Begin to move to BG\n",
      "Starting job 70a13ec2-fe5f-4915-a429-50fb5a5f7417\n",
      "Job finished.\n",
      "Year 2008 finished in 46.40419200261434 minutes\n",
      "data_2009.rar \n",
      " raw_data/data2009.rar\n",
      "Blob raw_data/data2009.rar downloaded to /Users/Thomas/Downloads/Stata_translate/data_2009.rar.\n",
      "/Users/Thomas/Downloads/Stata_translate/sup_2007\n",
      "Begin translate /Users/Thomas/Downloads/Stata_translate/01_translate.sh\n",
      "imp_exp\n",
      "出口    1.197784e+12\n",
      "进口    1.000723e+12\n",
      "Name: value, dtype: float64\n",
      "(11143059, 12)\n",
      "There are 3802762 missing Business type in the Dataset\n",
      "Business type is filled at 95% with 3624337 values retrieve from company name.\n",
      " The missing 178425 are filling with \"其他\" \n",
      " The dataset has 11143059 rows\n",
      "intermediate\n",
      "No     6228494\n",
      "Yes    4914565\n",
      "Name: intermediate, dtype: int64\n",
      "Sum missing cities 497282\n",
      "Exemple of missing cities ['安徽省滁县' '广东省番禹' '安徽省宣城' '山东省惠民' '河北省其它']\n",
      "178580 cities have been retrieved .\n",
      "Missing cities have now 35.9% filled values. \n",
      "We will try to retrieve 318702 missing cities from another \n",
      " list\n",
      "443907 cities have been retrieved .\n",
      "Missing cities have now 89.3% filled values. \n",
      "We will try to retrieve 53375 missing cities from another \n",
      " list\n",
      "53375 cities have been retrieved from another list. Those unmatchedin another list are filled qith \"其他\"\n",
      "City has now 100% filled values. \n",
      "There are 0 unknow cities\n",
      "Date                      0\n",
      "ID                        0\n",
      "Business_type             0\n",
      "Trade_type                0\n",
      "imp_exp                   0\n",
      "HS                        0\n",
      "Origin_and_destination    0\n",
      "value                     0\n",
      "Quantity                  0\n",
      "intermediate              0\n",
      "city_prod                 0\n",
      "matching_city             0\n",
      "dtype: int64\n",
      "imp_exp\n",
      "出口    1.197784e+12\n",
      "进口    1.000723e+12\n",
      "Name: value, dtype: float64\n",
      "0 rows have been removed from ID due to wrong formating\n",
      "Unique HS \n",
      " imp_exp\n",
      "出口    4904\n",
      "进口    4907\n",
      "Name: HS, dtype: int64 \n",
      " unique countries \n",
      " imp_exp\n",
      "出口    234\n",
      "进口    218\n",
      "Name: Origin_and_destination, dtype: int64\n",
      "imp_exp\n",
      "出口    1197783834077\n",
      "进口    1000723486234\n",
      "Name: value, dtype: int64\n",
      "Begin to upload to GCS\n",
      "File /Users/Thomas/Downloads/Stata_translate/data_2009.gz uploaded to gz_files/data_2009.gz.\n",
      "Begin to move to BG\n",
      "Starting job d15aa264-a957-413a-966f-25fcafa60e9d\n",
      "Job finished.\n",
      "Year 2009 finished in 51.23053786357244 minutes\n",
      "data_2010.rar \n",
      " raw_data/data2010.rar\n",
      "Blob raw_data/data2010.rar downloaded to /Users/Thomas/Downloads/Stata_translate/data_2010.rar.\n",
      "/Users/Thomas/Downloads/Stata_translate/sup_2007\n",
      "Begin translate /Users/Thomas/Downloads/Stata_translate/01_translate.sh\n",
      "imp_exp\n",
      "出口    1.573112e+12\n",
      "进口    1.374978e+12\n",
      "Name: value, dtype: float64\n",
      "(13114425, 12)\n",
      "There are 6563929 missing Business type in the Dataset\n",
      "Business type is filled at 97% with 6358704 values retrieve from company name.\n",
      " The missing 205225 are filling with \"其他\" \n",
      " The dataset has 13114425 rows\n",
      "intermediate\n",
      "No     7232617\n",
      "Yes    5881808\n",
      "Name: intermediate, dtype: int64\n",
      "Sum missing cities 535425\n",
      "Exemple of missing cities ['广东省其它' '广东省番禹' '浙江省其它' '浙江省黄岩' '山东省荣城']\n",
      "132518 cities have been retrieved .\n",
      "Missing cities have now 24.8% filled values. \n",
      "We will try to retrieve 402907 missing cities from another \n",
      " list\n",
      "477408 cities have been retrieved .\n",
      "Missing cities have now 89.2% filled values. \n",
      "We will try to retrieve 58017 missing cities from another \n",
      " list\n",
      "58017 cities have been retrieved from another list. Those unmatchedin another list are filled qith \"其他\"\n",
      "City has now 100% filled values. \n",
      "There are 0 unknow cities\n",
      "Date                      0\n",
      "ID                        0\n",
      "Business_type             0\n",
      "Trade_type                0\n",
      "imp_exp                   0\n",
      "HS                        0\n",
      "Origin_and_destination    0\n",
      "value                     0\n",
      "Quantity                  0\n",
      "intermediate              0\n",
      "city_prod                 0\n",
      "matching_city             0\n",
      "dtype: int64\n",
      "imp_exp\n",
      "出口    1.573112e+12\n",
      "进口    1.374978e+12\n",
      "Name: value, dtype: float64\n",
      "0 rows have been removed from ID due to wrong formating\n",
      "Unique HS \n",
      " imp_exp\n",
      "出口    4909\n",
      "进口    4911\n",
      "Name: HS, dtype: int64 \n",
      " unique countries \n",
      " imp_exp\n",
      "出口    234\n",
      "进口    218\n",
      "Name: Origin_and_destination, dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imp_exp\n",
      "出口    1573111692574\n",
      "进口    1374977620152\n",
      "Name: value, dtype: int64\n",
      "Begin to upload to GCS\n",
      "File /Users/Thomas/Downloads/Stata_translate/data_2010.gz uploaded to gz_files/data_2010.gz.\n",
      "Begin to move to BG\n",
      "Starting job 65e41828-dc38-49fc-8f78-d9e5470975d6\n",
      "Job finished.\n",
      "Year 2010 finished in 50.94630013306936 minutes\n",
      "8.491316455602647\n"
     ]
    }
   ],
   "source": [
    "list_year  = range(2003, 2011)\n",
    "\n",
    "batch_model(list_year, download = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Job finished in 494 minutes, so 8 hours. Partly due to slow connection\n",
    "\n",
    "# Download .gz locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_year  = range(2000, 2011)\n",
    "for i in list_year: \n",
    "    file_name = 'data_' + str(i) + '.gz'\n",
    "    source_blob_name = 'gz_files/data_'+ str(i) + '.gz'\n",
    "    print(file_name, '\\n',\n",
    "        source_blob_name)\n",
    "    download_blob(bucket_name = 'store_data_trade',\n",
    "                source_blob_name = source_blob_name,\n",
    "                destination_file_name =file_name)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "0.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
